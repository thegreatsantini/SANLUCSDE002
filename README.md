# SANLUCSDE002

Before writing my tests I took some time to think about what I was doing and what could be tested. I decided that the best way to test my web scrapping functions was to test the data that was written to the output file. Before testing the data I added some tests to the goodreads link, to make sure that we are getting a response with data. I thought this was important because if the api isn't working then the rest of the tests will fail but not because there is something wrong with the functions but error started with the api response. After testing the endpoint, I moved to the output data, I tested the data using multiple tests started very broad and gradualy getting more specific. Before testing the output file I had to run the script before the tests and use node's file system to read the output file.  This gave me some issues with asynchronicity becuase if I tried to read the output file after the guetQuotes decalaration it would fail because node would try to read the file before the functino finished running. I thought of two ways to solve this issua, the first was the read the output file within each test or to place a .then after getQuotes fucntion call and read the file there assigning the data to a variable outside of the promise, I decided to do the later so that I was only reading the file once. I first tested if the data was an array and an array with a length of 10 becuase that was how many posts I was supposed to scrape. I then made sure each value in the array was a an object with the expected keys because while the values will changes with each post all the objects should have the same keys. On After testing the getQuotes function I moved on to testing the facebook script. I chose to follow a simliar logic to test my facebok scraping script, testing the shape of the data and making sure the the objects have the correct keys. 

My next step was to create the goodreads quotes scraping and command line prompt functionality. To create the prompt functionality I used Node's built in read line functionality, this posts a message for the user and allows the to input test into the CLI. THis function authenticates users to goodreads, becuase the user might not have a profile or might might input incorrect login information the function will have to re-run untill the user is authenticated. I created this functionality using a two seperate functions. One function prompts the user to input login information, sanitizes the input and calls and authenitcate function that calls the goodreads endpoint to authenticate the user. The original contains three arguments ( authenticated <Bool>, username<String> and password <String>  ). the Boolean is used by the authentical function so that when it recursivly calls the prompt user function, it will send a boolean letting it know if the user is authenticated or not. If the user is not authenticated it will reprompt the user and call authenticate user again with the new information. However, if the function is called with a true boolean, the function with call getQuotes function and close the read line prompt method. 

getQuotes is the function I created to scrape goodreads for Mark Twains top quotes, to do this I used the axios to make the request to goodreads to get the HTML and cherrio package to parse the DOM for scraping and cherrios DOM tranversal methods to grab the elements that I needed. I used axios becuase of my familiaty with it and cherrio because that is what I have used in the past to scrape websites and I prefer it becuase the way that they implemented DOM tranversal porposefully similar to jQuery, which is a very clean and easy to use. The logic I followed to scrape the quotes was to leverage the .eq() method that cherrio provides. Essentially this method will grab the HTML element in the index of the node-list for that selector. For example if there are 10 divs with a class of quotes and I where to select the element using class='quotes' I would get a node-list of length 10, and if I where to use .eq(5) I would grab the 4th div in that list (it is 0 indexed). I one reason I leveraged this method was because I only needed to scrape the top 10 quotes, so I simply used a for loop that counts from 0 to 10. In each loop I gradded the corresponding quote, tags and votes for the node-list in that corresonding loop. At the start of the loop I instanctaited an empty currentPost object and placed the three items in the object with the coresponding keys then pushed it to the postsArray that will contain all the scrapped posts(10). Before placing it into the object I had to clean up the scraped data to take out extra space, new lines (\n), extra words and HTML encoded elements (e.g &ldquo; , &#8213) for the quote text I also used regex to place right and left double qoutes. Once the data had been gathered I uses Node's built in file creating and writing API to create the quotes.txt. 